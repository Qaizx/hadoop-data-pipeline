![CI](https://github.com/Ronnagon-Phukahuta/hadoop-data-pipeline/actions/workflows/ci.yml/badge.svg)

# Finance ITSC Dashboard

à¸£à¸°à¸šà¸š Data Lake à¹à¸¥à¸° Dashboard à¸ªà¸³à¸«à¸£à¸±à¸šà¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‡à¸šà¸›à¸£à¸°à¸¡à¸²à¸“ ITSC à¸¡à¸«à¸²à¸§à¸´à¸—à¸¢à¸²à¸¥à¸±à¸¢à¹€à¸Šà¸µà¸¢à¸‡à¹ƒà¸«à¸¡à¹ˆ

## Architecture

```
flowchart TD
    subgraph Input
        A[ğŸ“Š Excel / CSV]
        B[ğŸ¤– GPT\nColumn Fixer]
    end

    subgraph HDFS["HDFS Data Lake"]
        C[ğŸ“ Raw Zone\n/datalake/raw]
        D[ğŸ“ Staging Zone\n/datalake/staging]
        E[ğŸ“ Curated Zone\n/datalake/curated]
    end

    subgraph ETL["ETL Layer (PySpark)"]
        F[âš¡ Spark Job\nfinance_itsc_pipeline.py]
        G{Data Quality\nChecks}
        H[âœ… .done marker]
        I[âŒ .failed marker]
        J[ğŸ“§ Email Alert]
    end

    subgraph Orchestration
        K[ğŸŒ€ Airflow DAG\nevery 5 min]
    end

    subgraph Serving["Serving Layer (Hive)"]
        L[(ğŸ Hive\nWide Table)]
        M[(ğŸ Hive\nLong Table)]
    end

    subgraph Dashboard["Dashboard (Streamlit)"]
        N[ğŸ“ˆ Charts\nPlotly]
        O[ğŸ’¬ NLP Query\nThai â†’ HiveQL]
        P[ğŸ” Auth]
    end

    subgraph Infra
        Q[ğŸ”’ Nginx\nHTTPS Proxy]
        R[ğŸ³ Docker Compose]
    end

    A --> B --> C
    K --> F
    C --> F
    F --> G
    G -->|Pass| H
    G -->|Fail| I --> J
    H --> L
    L --> M
    L --> N
    L --> O
    O -->|GPT| O
    N --> Q
    O --> Q
    P --> Q
    R -.->|runs| HDFS
    R -.->|runs| ETL
    R -.->|runs| Dashboard
    R -.->|runs| Orchestration
```

**Stack**
- **Data Lake**: Hadoop HDFS + Hive Metastore
- **ETL**: Apache Spark (PySpark)
- **Orchestration**: Apache Airflow
- **Dashboard**: Streamlit + Plotly
- **NLP**: OpenAI GPT â†’ HiveQL
- **Proxy**: Nginx (HTTPS)

## Project Structure

```
HADOOP_NEW/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/               # Airflow DAGs
â”‚   â””â”€â”€ Dockerfile.airflow
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ components/         # Streamlit UI components
â”‚   â”œâ”€â”€ services/           # Hive + GPT integration
â”‚   â”œâ”€â”€ utils/              # History, helpers
â”‚   â”œâ”€â”€ app.py              # Entry point
â”‚   â”œâ”€â”€ auth.py             # Authentication
â”‚   â””â”€â”€ config.py           # Table schema, category mapping
â”œâ”€â”€ jobs/
â”‚   â””â”€â”€ finance_itsc_pipeline.py  # Spark ETL + Data Quality
â”œâ”€â”€ tests/                  # Unit tests (pytest)
â”œâ”€â”€ certs/                  # SSL certificates (à¹„à¸¡à¹ˆ commit)
â”œâ”€â”€ data/                   # Raw data files (à¹„à¸¡à¹ˆ commit)
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ nginx.conf
â””â”€â”€ .env                    # à¹„à¸¡à¹ˆ commit â€” à¸”à¸¹ .env.example
```

## Prerequisites

- Docker + Docker Compose
- OpenAI API Key
- Gmail App Password (à¸ªà¸³à¸«à¸£à¸±à¸š email alerts)

## Setup

**1. Clone à¹à¸¥à¸°à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² environment**
```bash
git clone <repo-url>
cd HADOOP_NEW
cp .env.example .env
# à¹à¸à¹‰à¹„à¸‚ .env à¹ƒà¸ªà¹ˆà¸„à¹ˆà¸²à¸ˆà¸£à¸´à¸‡
```

**2. à¸ªà¸£à¹‰à¸²à¸‡ SSL Certificate**
```bash
# Windows (Git Bash)
bash generate_cert.sh

# Linux/Mac
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
    -keyout certs/server.key \
    -out certs/server.crt \
    -subj "/C=TH/ST=ChiangMai/O=ITSC-CMU/CN=localhost"
```

**3. à¸ªà¸£à¹‰à¸²à¸‡ config.py à¸ˆà¸²à¸ example**
```bash
cp dashboard/config.py.example dashboard/config.py
# à¹à¸à¹‰à¹„à¸‚ config.py à¸•à¸²à¸¡à¸•à¹‰à¸­à¸‡à¸à¸²à¸£
```

**4. à¸£à¸±à¸™ Docker Compose**
```bash
docker compose up -d
```

**5. à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² Airflow**
```bash
# à¹€à¸‚à¹‰à¸² Airflow UI: http://localhost:8088
# Admin â†’ Variables â†’ à¹€à¸à¸´à¹ˆà¸¡:
#   Key: alert_email
#   Value: your-email@gmail.com
```

**6. Upload à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸‚à¹‰à¸² HDFS**
```bash
# à¸ªà¸£à¹‰à¸²à¸‡ directory structure
docker exec namenode hdfs dfs -mkdir -p /datalake/raw/finance-itsc/year=2024

# Upload CSV
docker exec -i namenode hdfs dfs -put /data/finance_itsc_2024.csv \
    /datalake/raw/finance-itsc/year=2024/
```

## Services

| Service | URL | à¸«à¸¡à¸²à¸¢à¹€à¸«à¸•à¸¸ |
|---------|-----|---------|
| Dashboard | https://localhost | à¸«à¸™à¹‰à¸²à¸«à¸¥à¸±à¸ |
| Airflow | http://localhost:8088 | Pipeline management |
| Spark Master | http://localhost:8080 | à¸«à¸£à¸·à¸­ https://localhost/spark/ |
| HDFS NameNode | http://localhost:9870 | |
| Hive Server | localhost:10000 | JDBC |

## ETL Pipeline

Pipeline à¸£à¸±à¸™à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´à¸—à¸¸à¸ 5 à¸™à¸²à¸—à¸µ à¸œà¹ˆà¸²à¸™ Airflow DAG `finance_etl_pipeline`

**Flow:**
```
1. à¸•à¸£à¸§à¸ˆ HDFS à¸«à¸²à¹„à¸Ÿà¸¥à¹Œà¹ƒà¸«à¸¡à¹ˆ (à¹„à¸¡à¹ˆà¸¡à¸µ .done marker)
2. Data Quality checks (schema, null, date format, total amount)
3. à¸–à¹‰à¸²à¸œà¹ˆà¸²à¸™ â†’ load à¹€à¸‚à¹‰à¸² Hive Wide table â†’ à¸ªà¸£à¹‰à¸²à¸‡ .done
4. à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸œà¹ˆà¸²à¸™ â†’ à¸ªà¸£à¹‰à¸²à¸‡ .failed â†’ à¸ªà¹ˆà¸‡ email alert
5. à¹à¸›à¸¥à¸‡ Wide â†’ Long format
```

**Marker files:**
- `filename.csv.done` â€” processed à¸ªà¸³à¹€à¸£à¹‡à¸ˆ
- `filename.csv.failed` â€” Data Quality failed (à¸•à¹‰à¸­à¸‡à¹à¸à¹‰à¹„à¸‚à¸à¹ˆà¸­à¸™ retry)

## Data Quality Checks

| Check | à¸£à¸°à¸”à¸±à¸š | à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸” |
|-------|-------|-----------|
| Schema | Fatal | Column à¸„à¸£à¸š 32 à¸­à¸±à¸™ |
| Null Values | Fatal | date, details à¸«à¹‰à¸²à¸¡ null |
| Date Format | Fatal | à¸•à¹‰à¸­à¸‡à¸¡à¸µ all-year-budget, total spent, remaining |
| Total Amount | Warning | total_amount â‰ˆ sum à¸—à¸¸à¸ column (Â±1%) |
| Remaining | Warning | remaining à¸•à¹‰à¸­à¸‡à¸¥à¸”à¸«à¸¥à¸±à¹ˆà¸™à¸—à¸¸à¸à¹€à¸”à¸·à¸­à¸™ |

## Running Tests

```bash
pytest tests/ -v
```

## Troubleshooting

**Spark à¹ƒà¸Šà¹‰ Python à¸œà¸´à¸” version**
```bash
# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š PYSPARK_PYTHON à¹ƒà¸™ docker-compose.yaml
- PYSPARK_PYTHON=python3
- PYSPARK_DRIVER_PYTHON=python3
```

**Hive reserved keyword error**
```
Pipeline à¸ˆà¸° auto-fix `date` â†’ `\`date\`` à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´
```

**HDFS à¹„à¸¡à¹ˆà¸‚à¸¶à¹‰à¸™**
```bash
docker compose restart namenode datanode
```

**Dashboard à¹„à¸¡à¹ˆà¸­à¸±à¸à¹€à¸”à¸—à¸«à¸¥à¸±à¸‡à¹à¸à¹‰à¹‚à¸„à¹‰à¸”**
```bash
docker compose restart streamlit-dashboard
```