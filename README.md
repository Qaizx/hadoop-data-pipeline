[![CI](https://github.com/Qaizx/hadoop-data-pipeline/actions/workflows/ci.yml/badge.svg)](https://github.com/Qaizx/hadoop-data-pipeline/actions/workflows/ci.yml)

# Finance ITSC Dashboard

à¸£à¸°à¸šà¸š Data Lake à¹à¸¥à¸° Dashboard à¸ªà¸³à¸«à¸£à¸±à¸šà¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‡à¸šà¸›à¸£à¸°à¸¡à¸²à¸“ ITSC à¸¡à¸«à¸²à¸§à¸´à¸—à¸¢à¸²à¸¥à¸±à¸¢à¹€à¸Šà¸µà¸¢à¸‡à¹ƒà¸«à¸¡à¹ˆ

## Architecture

```mermaid
flowchart TD
    subgraph Input
        A[ğŸ“Š Excel / CSV]
        B[ğŸ¤– GPT <br/>Column Fixer]
    end

    subgraph HDFS["HDFS Data Lake"]
        C[ğŸ“ Raw Zone<br/>/datalake/raw]
        D[ğŸ“ Staging Zone<br/>/datalake/staging]
        E[ğŸ“ Curated Zone<br/>/datalake/curated]
        V[ğŸ“¦ Versions<br/>/datalake/versions]
    end

    subgraph ETL["ETL Layer (PySpark)"]
        F[âš¡ Spark Job<br/>finance_itsc_pipeline.py]
        G{Data Quality<br/>Checks}
        AW[ğŸ”’ Atomic Write<br/>Swap Pattern]
        H[âœ… .done marker]
        I[âŒ .failed marker]
        J[ğŸ“§ Email Alert]
    end

    subgraph Orchestration
        K[ğŸŒ€ Airflow DAG <br/> every 5 min]
    end

    subgraph Serving["Serving Layer (Hive)"]
        L[(ğŸ Hive<br/>Wide Table)]
        M[(ğŸ Hive<br/>Long Table)]
    end

    subgraph Dashboard["Dashboard (Streamlit)"]
        N[ğŸ“ˆ Charts<br/>Plotly]
        O[ğŸ’¬ NLP Query<br/>Thai â†’ HiveQL]
        P[ğŸ” Auth]
    end

    subgraph Infra
        Q[ğŸ”’ Nginx<br/>HTTPS Proxy]
        R[ğŸ³ Docker Compose]
    end

    A --> B --> C
    K --> F
    C --> F
    F --> G
    G -->|Pass| AW
    G -->|Fail| I --> J
    AW --> H
    AW --> V
    H --> L
    L --> M
    L --> N
    L --> O
    O -->|GPT| O
    N --> Q
    O --> Q
    P --> Q
    R -.->|runs| HDFS
    R -.->|runs| ETL
    R -.->|runs| Dashboard
    R -.->|runs| Orchestration
```

**Stack**
- **Data Lake**: Hadoop HDFS + Hive Metastore
- **ETL**: Apache Spark (PySpark)
- **Orchestration**: Apache Airflow
- **Dashboard**: Streamlit + Plotly
- **NLP**: OpenAI GPT â†’ HiveQL
- **Proxy**: Nginx (HTTPS)

## Project Structure

```
HADOOP_NEW/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/               # Airflow DAGs
â”‚   â””â”€â”€ Dockerfile.airflow
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ components/         # Streamlit UI components
â”‚   â”œâ”€â”€ services/           # Hive + GPT integration
â”‚   â”œâ”€â”€ utils/              # History, helpers
â”‚   â”œâ”€â”€ app.py              # Entry point
â”‚   â”œâ”€â”€ auth.py             # Authentication
â”‚   â””â”€â”€ config.py           # Table schema, category mapping
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ finance_itsc_pipeline.py   # Spark ETL entry point
â”‚   â”œâ”€â”€ data_quality.py            # Data Quality checks
â”‚   â”œâ”€â”€ logger.py                  # Structured logging (loguru)
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ hdfs.py                # HDFS helpers
â”‚       â”œâ”€â”€ alerts.py              # Email alerts
â”‚       â”œâ”€â”€ retry.py               # Retry + Atomic write
â”‚       â””â”€â”€ versioning.py          # Data versioning / rollback
â”œâ”€â”€ tests/                  # Unit tests (pytest)
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ versioning.md       # à¸„à¸¹à¹ˆà¸¡à¸·à¸­ versioning à¹à¸¥à¸° rollback
â”œâ”€â”€ certs/                  # SSL certificates (à¹„à¸¡à¹ˆ commit)
â”œâ”€â”€ data/                   # Raw data files (à¹„à¸¡à¹ˆ commit)
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ nginx.conf
â””â”€â”€ .env                    # à¹„à¸¡à¹ˆ commit â€” à¸”à¸¹ .env.example
```

## Prerequisites

- Docker + Docker Compose
- OpenAI API Key
- Gmail App Password (à¸ªà¸³à¸«à¸£à¸±à¸š email alerts)

## Setup

**1. Clone à¹à¸¥à¸°à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² environment**
```bash
git clone <repo-url>
cd HADOOP_NEW
cp .env.example .env
# à¹à¸à¹‰à¹„à¸‚ .env à¹ƒà¸ªà¹ˆà¸„à¹ˆà¸²à¸ˆà¸£à¸´à¸‡
```

**2. à¸ªà¸£à¹‰à¸²à¸‡ SSL Certificate**
```bash
# Windows (Git Bash)
bash generate_cert.sh

# Linux/Mac
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
    -keyout certs/server.key \
    -out certs/server.crt \
    -subj "/C=TH/ST=ChiangMai/O=ITSC-CMU/CN=localhost"
```

**3. à¸ªà¸£à¹‰à¸²à¸‡ config.py à¸ˆà¸²à¸ example**
```bash
cp dashboard/config.py.example dashboard/config.py
# à¹à¸à¹‰à¹„à¸‚ config.py à¸•à¸²à¸¡à¸•à¹‰à¸­à¸‡à¸à¸²à¸£
```

**4. à¸£à¸±à¸™ Docker Compose**
```bash
docker compose up -d
```

**5. à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² Airflow**
```bash
# à¹€à¸‚à¹‰à¸² Airflow UI: http://localhost:8088
# Admin â†’ Variables â†’ à¹€à¸à¸´à¹ˆà¸¡:
#   Key: alert_email
#   Value: your-email@gmail.com
```

**6. Upload à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸‚à¹‰à¸² HDFS**
```bash
# à¸ªà¸£à¹‰à¸²à¸‡ directory structure
docker exec namenode hdfs dfs -mkdir -p /datalake/raw/finance-itsc/year=2024

# Upload CSV
docker exec -i namenode hdfs dfs -put /data/finance_itsc_2024.csv \
    /datalake/raw/finance-itsc/year=2024/
```

## Environment Variables

à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¹ƒà¸™ `.env`:

| Variable | Default | Description |
|----------|---------|-------------|
| `ETL_MAX_RETRIES` | `3` | à¸ˆà¸³à¸™à¸§à¸™à¸„à¸£à¸±à¹‰à¸‡ retry à¹€à¸¡à¸·à¹ˆà¸­ step fail |
| `ETL_RETRY_DELAY` | `5` | à¸§à¸´à¸™à¸²à¸—à¸µà¸£à¸­à¸à¹ˆà¸­à¸™ retry (x2 à¸—à¸¸à¸à¸£à¸­à¸š) |
| `KEEP_VERSIONS` | `5` | à¸ˆà¸³à¸™à¸§à¸™ version à¸—à¸µà¹ˆà¹€à¸à¹‡à¸šà¸•à¹ˆà¸­à¸›à¸µ |
| `LOG_DIR` | `/jobs/logs` | path à¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸à¹‡à¸š log files |

## Services

| Service | URL | à¸«à¸¡à¸²à¸¢à¹€à¸«à¸•à¸¸ |
|---------|-----|---------|
| Dashboard | https://localhost | à¸«à¸™à¹‰à¸²à¸«à¸¥à¸±à¸ |
| Airflow | http://localhost:8088 | Pipeline management |
| Spark Master | http://localhost:8080 | à¸«à¸£à¸·à¸­ https://localhost/spark/ |
| HDFS NameNode | http://localhost:9870 | |
| Hive Server | localhost:10000 | JDBC |

## ETL Pipeline

Pipeline à¸£à¸±à¸™à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´à¸—à¸¸à¸ 5 à¸™à¸²à¸—à¸µ à¸œà¹ˆà¸²à¸™ Airflow DAG `finance_etl_pipeline`

```mermaid
flowchart TD
    A([ğŸŒ€ Airflow trigger]) --> B[Scan HDFS<br/>à¸«à¸²à¹„à¸Ÿà¸¥à¹Œà¹ƒà¸«à¸¡à¹ˆ]
    B --> B1{à¸à¸šà¹„à¸Ÿà¸¥à¹Œ<br/>à¹ƒà¸«à¸¡à¹ˆ?}
    B1 -->|à¹„à¸¡à¹ˆà¸¡à¸µ| Z([â­ï¸ Skip])
    B1 -->|à¸¡à¸µ| C[Read CSV]

    C --> C1{à¸ªà¸³à¹€à¸£à¹‡à¸ˆ?}
    C1 -->|Fail| C2[Retry<br/>5â†’10â†’20 à¸§à¸´]
    C2 -->|à¸«à¸¡à¸” retry| FAIL1([âŒ Skip à¸›à¸µà¸™à¸µà¹‰])
    C2 -->|à¸ªà¸³à¹€à¸£à¹‡à¸ˆ| E

    C1 -->|Pass| E
    E[Data Quality Checks<br/>schema, null, date, total]
    E --> E1{à¸œà¹ˆà¸²à¸™?}
    E1 -->|Fail| E2[à¸ªà¸£à¹‰à¸²à¸‡ .failed<br/>ğŸ“§ Alert]
    E2 --> FAIL2([âŒ Skip à¸›à¸µà¸™à¸µà¹‰])
    E1 -->|Pass| F

    F[Atomic Write<br/>Staging Wide Table]
    F --> F1{à¸ªà¸³à¹€à¸£à¹‡à¸ˆ?}
    F1 -->|Fail| F2[Retry + Swap<br/>Rollback à¸–à¹‰à¸² crash]
    F2 -->|à¸«à¸¡à¸” retry| FAIL3([âŒ Skip à¸›à¸µà¸™à¸µà¹‰])
    F2 -->|à¸ªà¸³à¹€à¸£à¹‡à¸ˆ| G

    F1 -->|Pass| G[à¸ªà¸£à¹‰à¸²à¸‡ .done<br/>ğŸ“¸ Snapshot Version]
    G --> H[Atomic Write<br/>Curated Long Table]
    H --> H1{à¸ªà¸³à¹€à¸£à¹‡à¸ˆ?}
    H1 -->|Fail| H2[Retry + Swap<br/>Rollback à¸–à¹‰à¸² crash]
    H2 -->|à¸«à¸¡à¸” retry| FAIL4([âš ï¸ Wide OK, Long fail])
    H2 -->|à¸ªà¸³à¹€à¸£à¹‡à¸ˆ| DONE
    H1 -->|Pass| DONE([âœ… Done])
```

à¸—à¸¸à¸ step à¸¡à¸µ retry à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´à¸à¸£à¹‰à¸­à¸¡ exponential backoff (5 â†’ 10 â†’ 20 à¸§à¸´à¸™à¸²à¸—à¸µ)

**Marker files:**
- `filename.csv.done` â€” processed à¸ªà¸³à¹€à¸£à¹‡à¸ˆ
- `filename.csv.failed` â€” Data Quality failed (à¸•à¹‰à¸­à¸‡à¹à¸à¹‰à¹„à¸‚à¸à¹ˆà¸­à¸™ retry)

## Data Quality Checks

| Check | à¸£à¸°à¸”à¸±à¸š | à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸” |
|-------|-------|-----------|
| Schema | Fatal | Column à¸„à¸£à¸š 32 à¸­à¸±à¸™ |
| Null Values | Fatal | date, details à¸«à¹‰à¸²à¸¡ null |
| Date Format | Fatal | à¸•à¹‰à¸­à¸‡à¸¡à¸µ all-year-budget, total spent, remaining |
| Total Amount | Warning | total_amount â‰ˆ sum à¸—à¸¸à¸ column (Â±1%) |
| Remaining | Warning | remaining à¸•à¹‰à¸­à¸‡à¸¥à¸”à¸«à¸¥à¸±à¹ˆà¸‡à¸—à¸¸à¸à¹€à¸”à¸·à¸­à¸™ |

## Atomic Write & Retry

à¸›à¹‰à¸­à¸‡à¸à¸±à¸™ partial data à¹€à¸‚à¹‰à¸² Hive table à¸”à¹‰à¸§à¸¢ **swap pattern** â€” à¹€à¸‚à¸µà¸¢à¸™à¹à¸¢à¸ partition à¹€à¸‰à¸à¸²à¸°à¸›à¸µà¸—à¸µà¹ˆ process à¸›à¸µà¸­à¸·à¹ˆà¸™à¹„à¸¡à¹ˆà¹‚à¸”à¸™à¹à¸•à¸°

```mermaid
flowchart TD
    A([à¹€à¸£à¸´à¹ˆà¸¡ Atomic Write<br/>year=2024]) --> B[à¹€à¸‚à¸µà¸¢à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¥à¸‡<br/>year=2024_tmp]

    B --> B1{à¸ªà¸³à¹€à¸£à¹‡à¸ˆ?}
    B1 -->|Fail| B2[à¸¥à¸š _tmp à¸—à¸´à¹‰à¸‡<br/>table à¹€à¸”à¸´à¸¡à¸¢à¸±à¸‡à¸­à¸¢à¸¹à¹ˆà¸„à¸£à¸š]
    B2 --> RETRY([ğŸ”„ Retry])
    B1 -->|Pass| C

    C[rename<br/>year=2024 â†’ year=2024_old]
    C --> C1{à¸ªà¸³à¹€à¸£à¹‡à¸ˆ?}
    C1 -->|Fail| C2([âŒ Error<br/>table à¹€à¸”à¸´à¸¡à¸¢à¸±à¸‡à¸­à¸¢à¸¹à¹ˆà¸„à¸£à¸š])
    C1 -->|Pass| D

    D[rename<br/>year=2024_tmp â†’ year=2024]
    D --> D1{à¸ªà¸³à¹€à¸£à¹‡à¸ˆ?}
    D1 -->|Fail| D2[Rollback<br/>year=2024_old â†’ year=2024]
    D2 --> FAIL([âŒ Error<br/>à¸„à¸·à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸”à¸´à¸¡à¹ƒà¸«à¹‰à¹à¸¥à¹‰à¸§])
    D1 -->|Pass| E

    E[à¸¥à¸š year=2024_old]
    E --> DONE([âœ… Done<br/>year=2024 à¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸«à¸¡à¹ˆ<br/>year=2023, 2025 à¹„à¸¡à¹ˆà¹‚à¸”à¸™à¹à¸•à¸°])

    style B fill:#dbeafe
    style C fill:#fef9c3
    style D fill:#fef9c3
    style E fill:#dcfce7
    style DONE fill:#dcfce7
    style FAIL fill:#fee2e2
    style C2 fill:#fee2e2
```

## Data Versioning

à¸—à¸¸à¸à¸„à¸£à¸±à¹‰à¸‡à¸—à¸µà¹ˆ ETL à¸ªà¸³à¹€à¸£à¹‡à¸ˆà¸ˆà¸°à¸ªà¸£à¹‰à¸²à¸‡ snapshot à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´ à¹€à¸à¹‡à¸šà¹„à¸§à¹‰ **5 version à¸¥à¹ˆà¸²à¸ªà¸¸à¸”** à¸•à¹ˆà¸­à¸›à¸µ

**à¸”à¸¹ versions à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”:**
```python
from utils.versioning import list_versions
versions = list_versions(sc, year=2024)
for v in versions:
    print(f"{v['version']} | {v['timestamp']} | rows={v['row_count']}")
```

**Rollback à¹„à¸› version à¹€à¸à¹ˆà¸²:**
```python
from utils.versioning import restore_version
restore_version(
    spark,
    version_id="v_20260215_090000",
    year=2024,
    target_table="finance_itsc_wide",
    target_path="hdfs://namenode:8020/datalake/staging/finance-itsc_wide",
)
```

à¸”à¸¹à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡à¹„à¸”à¹‰à¸—à¸µà¹ˆ [docs/versioning.md](docs/versioning.md)

## Running Tests

```bash
# à¸£à¸±à¸™ test à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
pytest tests/ -v

# à¸£à¸±à¸™ test à¹€à¸‰à¸à¸²à¸° module
pytest tests/test_atomic_write.py -v
pytest tests/test_versioning.py -v
```

**Test coverage:**

| Test file | à¸—à¸”à¸ªà¸­à¸šà¸­à¸°à¹„à¸£ |
|-----------|-----------|
| `test_atomic_write.py` | Swap pattern, retry, rollback, à¸›à¸µà¸­à¸·à¹ˆà¸™à¹„à¸¡à¹ˆà¹‚à¸”à¸™à¹à¸•à¸° |
| `test_versioning.py` | Create snapshot, list versions, cleanup, restore |

## Troubleshooting

**Spark à¹ƒà¸Šà¹‰ Python à¸œà¸´à¸” version**
```bash
# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š PYSPARK_PYTHON à¹ƒà¸™ docker-compose.yaml
- PYSPARK_PYTHON=python3
- PYSPARK_DRIVER_PYTHON=python3
```

**Hive reserved keyword error**
```
Pipeline à¸ˆà¸° auto-fix `date` â†’ `\`date\`` à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´
```

**HDFS à¹„à¸¡à¹ˆà¸‚à¸¶à¹‰à¸™**
```bash
docker compose restart namenode datanode
```

**Dashboard à¹„à¸¡à¹ˆà¸­à¸±à¸à¹€à¸”à¸—à¸«à¸¥à¸±à¸‡à¹à¸à¹‰à¹‚à¸„à¹‰à¸”**
```bash
docker compose restart streamlit-dashboard
```

**à¸”à¸¹ logs à¸‚à¸­à¸‡ ETL pipeline**
```bash
# log à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
docker exec spark-master cat /jobs/logs/etl.log

# à¹€à¸‰à¸à¸²à¸° error
docker exec spark-master cat /jobs/logs/etl.error.log
```